{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install -q -U trl\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import needed moduls GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "raw_dataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\")\n",
    "\n",
    "# Display the dataset\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data sample\n",
    "train_data = raw_dataset['train']\n",
    "\n",
    "# Print the fields of the example\n",
    "print(\"Instruction:\", train_data[0][\"instruction\"])\n",
    "print(\"Input:\", train_data[0][\"input\"])\n",
    "print(\"Output:\", train_data[0][\"output\"])\n",
    "print(\"Prompt:\", train_data[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset with 90/10 ratio\n",
    "split_dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Creating the new DatasetDict with train and test splits and wrapping it back into a DatasetDict\n",
    "train_data = split_dataset[\"train\"]\n",
    "test_data = split_dataset[\"test\"]\n",
    "\n",
    "new_dataset_dict = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"test\": test_data\n",
    "})\n",
    "\n",
    "# Display the resulting splits\n",
    "new_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating smaller subsets for training (just 1000 samples to speedup training)\n",
    "debug_train_data_1k = new_dataset_dict[\"train\"].select(range(1000))\n",
    "debug_test_data_1k = new_dataset_dict[\"test\"].select(range(100))\n",
    "\n",
    "# Wrapping it into a new DatasetDict for debugging\n",
    "debug_dataset_dict_1k = DatasetDict({\n",
    "    \"train\": debug_train_data_1k,\n",
    "    \"test\": debug_test_data_1k\n",
    "})\n",
    "\n",
    "# Show the resulting debug DatasetDict\n",
    "debug_dataset_dict_1k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with Base Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flash_attention = False\n",
    "\n",
    "# Your Hugging Face model and configurations\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "mode_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# If you're using a different model that supports safetensors, you can use one like this:\n",
    "model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "# model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the model with safetensors\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_flash_attention_2=True,\n",
    "    trust_remote_code=True, \n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    return f\"\"\"### INSTRUCTION:\n",
    "You are an AI coding assistant specialized in generating Python code from user instructions. Your task is to return only the code that directly fulfills the given instruction.</s>\n",
    "\n",
    "### Input:\n",
    "{sample['instruction']}</s>\n",
    "\n",
    "### RESPONSE:\n",
    "{sample['output']}</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(user_input):\n",
    "    return f\"\"\"### INSTRUCTION:\n",
    "You are an AI coding assistant specialized in generating Python code from user instructions.\n",
    "Your task is to return only the code that directly fulfills the given instruction.</s>\n",
    "\n",
    "### Input:\n",
    "{user_input}</s>\n",
    "\n",
    "### RESPONSE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "model.config.use_cache = True\n",
    "\n",
    "prompt = generate_prompt(debug_dataset_dict_1k[\"test\"][0][\"instruction\"])\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_cache_and_collect():\n",
    "    \"\"\"\n",
    "    Perform garbage collection, clear CUDA cache, and delete model, trainer, and tokenizer\n",
    "    if they exist, without causing errors if they are not defined. Additionally, print CUDA\n",
    "    memory summary after clearing the cache.\n",
    "    \"\"\"\n",
    "    # Safely delete model, trainer, and tokenizer if they exist\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass  # If 'model' doesn't exist, do nothing\n",
    "    \n",
    "    try:\n",
    "        del trainer\n",
    "    except NameError:\n",
    "        pass  # If 'trainer' doesn't exist, do nothing\n",
    "    \n",
    "    try:\n",
    "        del tokenizer\n",
    "    except NameError:\n",
    "        pass  # If 'tokenizer' doesn't exist, do nothing\n",
    "\n",
    "    # Loop until garbage collection collects no objects\n",
    "    while True:\n",
    "        gc_collected = gc.collect()  # Perform garbage collection\n",
    "        torch.cuda.empty_cache()     # Clear the CUDA cache\n",
    "        \n",
    "        # Break the loop if no objects are collected\n",
    "        if gc_collected == 0:\n",
    "            break\n",
    "\n",
    "    # Clear the cache and print a summary of CUDA memory usage\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=0, abbreviated=True))\n",
    "    \n",
    "    print(\"Cache clearing, garbage collection, and variable deletion are complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache_and_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timed_training(trainer):\n",
    "    \"\"\"\n",
    "    Measures the time taken to train the model using the provided trainer.\n",
    "    \"\"\"\n",
    "    start_time = time.time() \n",
    "\n",
    "    trainer.train() \n",
    "\n",
    "    end_time = time.time()  \n",
    "    elapsed_time = end_time - start_time  \n",
    "\n",
    "    # Convert the elapsed time to hours, minutes, and seconds\n",
    "    hours, remainder = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    print(f\"Training completed in {int(hours)} hours, {int(minutes)} minutes, and {int(seconds)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flash_attention = False\n",
    "\n",
    "# Your Hugging Face model and configurations\n",
    "model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "\n",
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the model with safetensors\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_flash_attention_2=True,\n",
    "    trust_remote_code=True,  \n",
    "    use_safetensors=True,   \n",
    ")\n",
    "\n",
    "# Additional configuration\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Print a success message\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "                  \"q_proj\",\n",
    "                  \"up_proj\",\n",
    "                  \"o_proj\",\n",
    "                  \"k_proj\",\n",
    "                  \"down_proj\",\n",
    "                  \"gate_proj\",\n",
    "                  \"v_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_eval_batch_size=5,\n",
    "    per_device_train_batch_size=5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    save_strategy=\"epoch\", \n",
    "    eval_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,  \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    seed=42,\n",
    "    save_total_limit=1,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Add the EarlyStoppingCallback\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=debug_dataset_dict_1k[\"train\"],\n",
    "    eval_dataset=debug_dataset_dict_1k[\"test\"],\n",
    "    peft_config=config,\n",
    "    formatting_func=format_instruction,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.001)]\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed_training(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name = \"early_stopping_3_epoch_fine_tuned_laama\"\n",
    "trainer.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache_and_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16\n",
    "device_map=\"auto\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load fine-tuned model and merge with Adapter\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(debug_dataset_dict_1k[\"test\"][0][\"instruction\"])\n",
    "# Run text generation pipeline with our next model\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
